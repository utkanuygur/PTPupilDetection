{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "cellView": "form",
        "id": "ydENzGBBnp9x"
      },
      "outputs": [],
      "source": [
        "#@title Copy Dataset\n",
        "%%capture\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!cp drive/MyDrive/new_dataset.zip .\n",
        "drive.flush_and_unmount()\n",
        "!unzip new_dataset.zip\n",
        "!mv new_dataset/* .\n",
        "!rm -r new_dataset* sample_data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "cellView": "form",
        "id": "rbJdu6dpAwu9"
      },
      "outputs": [],
      "source": [
        "#@title Import Libraries\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms.functional as TF\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "import torchvision\n",
        "import cv2\n",
        "import os\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "from tqdm import tqdm\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "import gc\n",
        "import torchvision.transforms as transforms\n",
        "import torch.multiprocessing as mp\n",
        "import re\n",
        "from time import time\n",
        "import multiprocessing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "cellView": "form",
        "id": "gqw7Y5nYVSXA"
      },
      "outputs": [],
      "source": [
        "#@title Define Model and Loader\n",
        "\n",
        "class DoubleConv(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(DoubleConv, self).__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, 3, 1, 1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_channels, out_channels, 3, 1, 1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv(x)\n",
        "\n",
        "\n",
        "class UNET(nn.Module):\n",
        "    def __init__(self, in_channels=1, out_channels=1, features=[64, 128, 256, 512]):\n",
        "        super(UNET, self).__init__()\n",
        "        self.ups = nn.ModuleList()\n",
        "        self.downs = nn.ModuleList()\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        for feature in features:\n",
        "            self.downs.append(DoubleConv(in_channels, feature))\n",
        "            in_channels = feature\n",
        "\n",
        "        for feature in reversed(features):\n",
        "            self.ups.append(nn.ConvTranspose2d(feature * 2, feature, kernel_size=2, stride=2))\n",
        "            self.ups.append(DoubleConv(feature * 2, feature))\n",
        "\n",
        "        self.bottleneck = DoubleConv(features[-1], features[-1] * 2)\n",
        "        self.final_conv = nn.Conv2d(features[0], out_channels, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        skip_connections = []\n",
        "\n",
        "        for down in self.downs:\n",
        "            x = down(x)\n",
        "            skip_connections.append(x)\n",
        "            x = self.pool(x)\n",
        "\n",
        "        x = self.bottleneck(x)\n",
        "        skip_connections = skip_connections[::-1]\n",
        "\n",
        "        for idx in range(0, len(self.ups), 2):\n",
        "            x = self.ups[idx](x)\n",
        "            skip_connection = skip_connections[idx // 2]\n",
        "            if x.shape != skip_connection.shape:\n",
        "                x = TF.resize(x, size=skip_connection.shape[2:])\n",
        "\n",
        "            concat_skip = torch.cat((skip_connection, x), dim=1)\n",
        "            x = self.ups[idx + 1](concat_skip)\n",
        "\n",
        "        return self.final_conv(x)\n",
        "\n",
        "\n",
        "class PupilDataset(Dataset):\n",
        "    def __init__(self, image_dir, mask_dir, transform=None):\n",
        "        self.image_dir = image_dir\n",
        "        self.mask_dir = mask_dir\n",
        "        self.transform = transform\n",
        "        self.images = os.listdir(image_dir)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img_path = os.path.join(self.image_dir, self.images[index])\n",
        "        mask_path = os.path.join(self.mask_dir, self.images[index].replace(\".jpg\",\".gif\"))\n",
        "        image = np.array(Image.open(img_path).convert(\"L\"))\n",
        "        mask = np.array(Image.open(mask_path).convert(\"L\"),dtype=np.float32)\n",
        "        mask[mask == 255.0] = 1.0\n",
        "\n",
        "        if self.transform is not None:\n",
        "            augmentations = self.transform(image=image, mask=mask)\n",
        "            image = augmentations[\"image\"]\n",
        "            mask = augmentations[\"mask\"]\n",
        "\n",
        "        return image, mask\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "cellView": "form",
        "id": "HbrLF8g-VWV2"
      },
      "outputs": [],
      "source": [
        "#@title Define Helper Functions\n",
        "\n",
        "def save_checkpoint(state, filename=\"my_checkpoint.pth.tar\"):\n",
        "    print(\"=> Saving checkpoint\")\n",
        "    torch.save(state, filename)\n",
        "\n",
        "\n",
        "def load_checkpoint(checkpoint, model):\n",
        "    print(\"=> Loading checkpoint\")\n",
        "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
        "\n",
        "def get_loaders(\n",
        "    train_dir,\n",
        "    train_maskdir,\n",
        "    val_dir,\n",
        "    val_maskdir,\n",
        "    batch_size,\n",
        "    train_transform,\n",
        "    val_transform,\n",
        "    num_workers=4,\n",
        "    pin_memory=True,\n",
        "):\n",
        "    train_ds = PupilDataset(image_dir=train_dir, mask_dir=train_maskdir, transform=train_transform)\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        train_ds, batch_size=batch_size, num_workers=num_workers, pin_memory=pin_memory, shuffle=True\n",
        "    )\n",
        "    val_ds = PupilDataset(image_dir=val_dir, mask_dir=val_maskdir, transform=val_transform)\n",
        "\n",
        "    val_loader = DataLoader(\n",
        "        val_ds, batch_size=batch_size, num_workers=num_workers, pin_memory=pin_memory, shuffle=False\n",
        "    )\n",
        "\n",
        "    return train_loader, val_loader\n",
        "\n",
        "def check_accuracy(loader,model, device=\"cuda\"):\n",
        "    num_correct = 0\n",
        "    num_pixels = 0\n",
        "    dice_score = 0\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x,y in loader:\n",
        "            x = x.to(device)\n",
        "            y = y.to(device).unsqueeze(1)\n",
        "            preds = torch.sigmoid(model(x))\n",
        "            preds = (preds > 0.5).float()\n",
        "            num_correct += (preds == y).sum()\n",
        "            num_pixels += torch.numel(preds)\n",
        "            dice_score += (2 * (preds * y).sum()) / ((preds + y).sum() + 1e-8)\n",
        "    print(f\"Got {num_correct}/{num_pixels} with acc {num_correct/num_pixels*100:.2f}\")\n",
        "    print(f\"Dice score: {dice_score/len(loader)}\")\n",
        "    model.train()\n",
        "    return dice_score/len(loader)\n",
        "\n",
        "def save_predictions_as_imgs(\n",
        "    loader,model,folder=\"saved_images/\",device=\"cuda\"\n",
        "):\n",
        "    model.eval()\n",
        "    for idx, (x,y) in enumerate(loader):\n",
        "        x = x.to(device=device)\n",
        "        with torch.no_grad():\n",
        "            preds = torch.sigmoid(model(x))\n",
        "            preds = (preds > 0.5).float()\n",
        "        torchvision.utils.save_image(\n",
        "            preds, f\"{folder}/pred_{idx}.png\"\n",
        "        )\n",
        "        y = y.float() / y.max()\n",
        "        torchvision.utils.save_image(y.unsqueeze(1), f\"{folder}.png\")\n",
        "    model.train()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "8ONZSu09Vjup"
      },
      "outputs": [],
      "source": [
        "#@title Train the Model\n",
        "\n",
        "LEARNING_RATE = 1e-3\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "BATCH_SIZE = 40\n",
        "NUM_EPOCHS = 1000\n",
        "NUM_WORKERS = 8\n",
        "IMAGE_HEIGHT = 380\n",
        "IMAGE_WIDTH = 540\n",
        "PIN_MEMORY = True\n",
        "LOAD_MODEL = False\n",
        "TRAIN_IMG_DIR = \"train/images\"\n",
        "TRAIN_MASK_DIR = \"train/masks\"\n",
        "VAL_IMG_DIR = \"val/images\"\n",
        "VAL_MASK_DIR = \"val/masks\"\n",
        "\n",
        "def train_fn(loader, model, optimizer, bce_loss, scaler, epoch):\n",
        "    model.train()\n",
        "    total_bce_loss = 0\n",
        "\n",
        "    progress_bar = tqdm(loader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS} - Training\", leave=True)\n",
        "\n",
        "    for batch_idx, (data, targets) in enumerate(progress_bar):\n",
        "        data = data.to(DEVICE)\n",
        "        targets = targets.float().unsqueeze(1).to(DEVICE)\n",
        "\n",
        "        with torch.cuda.amp.autocast():\n",
        "            predictions = model(data)\n",
        "            bce = bce_loss(predictions, targets)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        scaler.scale(bce).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        total_bce_loss += bce.item()\n",
        "\n",
        "        progress_bar.set_postfix({\"BCE Loss\": f\"{bce.item():.4f}\"})\n",
        "\n",
        "    return total_bce_loss / len(loader)\n",
        "\n",
        "def validate_fn(loader, model, bce_loss, epoch):\n",
        "    model.eval()\n",
        "    total_bce_loss = 0\n",
        "\n",
        "    progress_bar = tqdm(loader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS} - Validation\", leave=True)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (data, targets) in enumerate(progress_bar):\n",
        "            data = data.to(DEVICE)\n",
        "            targets = targets.float().unsqueeze(1).to(DEVICE)\n",
        "\n",
        "            predictions = model(data)\n",
        "            bce = bce_loss(predictions, targets)\n",
        "\n",
        "            total_bce_loss += bce.item()\n",
        "\n",
        "            progress_bar.set_postfix({\"BCE Loss\": f\"{bce.item():.4f}\"})\n",
        "\n",
        "    return total_bce_loss / len(loader)\n",
        "\n",
        "def main():\n",
        "    train_transform = A.Compose(\n",
        "        [\n",
        "            A.Resize(height=IMAGE_HEIGHT, width=IMAGE_WIDTH),\n",
        "            A.Rotate(limit=5, p=0.5),\n",
        "            A.HorizontalFlip(p=0.25),\n",
        "            A.VerticalFlip(p=0.25),\n",
        "            A.RandomResizedCrop(height=IMAGE_HEIGHT, width=IMAGE_WIDTH, scale=(0.9, 1.0), ratio=(0.9, 1.1), p=0.2),\n",
        "            A.RandomBrightnessContrast(p=0.2),\n",
        "            A.RandomGamma(p=0.2),\n",
        "            A.Normalize(mean=0.0, std=1.0, max_pixel_value=255.0),\n",
        "            ToTensorV2(),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    val_transforms = A.Compose(\n",
        "        [\n",
        "            A.Resize(height=IMAGE_HEIGHT, width=IMAGE_WIDTH),\n",
        "            A.Normalize(mean=0.0, std=1.0, max_pixel_value=255.0),\n",
        "            ToTensorV2(),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    model = UNET(in_channels=1, out_channels=1).to(DEVICE)\n",
        "    bce_loss = nn.BCEWithLogitsLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n",
        "\n",
        "    train_loader, val_loader = get_loaders(\n",
        "        TRAIN_IMG_DIR, TRAIN_MASK_DIR, VAL_IMG_DIR, VAL_MASK_DIR, BATCH_SIZE, train_transform, val_transforms, NUM_WORKERS, PIN_MEMORY\n",
        "    )\n",
        "\n",
        "    best_bce = float('inf')\n",
        "\n",
        "    if LOAD_MODEL:\n",
        "        checkpoint = torch.load(\"my_checkpoint.pth.tar\")\n",
        "        load_checkpoint(checkpoint, model)\n",
        "        best_bce = checkpoint.get(\"best_bce\", float('inf'))\n",
        "\n",
        "    scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "    for epoch in range(NUM_EPOCHS):\n",
        "        train_bce_loss = train_fn(train_loader, model, optimizer, bce_loss, scaler, epoch)\n",
        "\n",
        "        current_bce = validate_fn(val_loader, model, bce_loss, epoch)\n",
        "\n",
        "        scheduler.step(current_bce)\n",
        "\n",
        "        if current_bce < best_bce:\n",
        "            best_bce = current_bce\n",
        "            print(f\"New best model found! Saving model with validation BCE loss: {best_bce:.4f}\")\n",
        "            checkpoint = {\n",
        "                \"state_dict\": model.state_dict(),\n",
        "                \"optimizer\": optimizer.state_dict(),\n",
        "                \"best_bce\": best_bce\n",
        "            }\n",
        "            save_checkpoint(checkpoint, filename=\"best_model.pth.tar\")\n",
        "\n",
        "        if epoch % 5 == 0:\n",
        "            checkpoint = {\n",
        "                \"state_dict\": model.state_dict(),\n",
        "                \"optimizer\": optimizer.state_dict(),\n",
        "                \"best_bce\": best_bce\n",
        "            }\n",
        "            save_checkpoint(checkpoint)\n",
        "\n",
        "        if not os.path.exists(\"saved_images\"):\n",
        "            os.makedirs(\"saved_images\")\n",
        "        save_predictions_as_imgs(val_loader, model, folder=\"saved_images\", device=DEVICE)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "TboKguE27MsD"
      },
      "outputs": [],
      "source": [
        "#@title Clear Cache and Display Memory\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "print(torch.cuda.memory_summary())\n",
        "print(f\"Memory Allocated: {torch.cuda.memory_allocated()} bytes\")\n",
        "print(f\"Memory Reserved: {torch.cuda.memory_reserved()} bytes\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "XUDSGgSGs0sN"
      },
      "outputs": [],
      "source": [
        "#@title Generate the Video\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "IMAGE_HEIGHT = 380\n",
        "IMAGE_WIDTH = 540\n",
        "BATCH_SIZE = 58\n",
        "NUM_WORKERS = multiprocessing.cpu_count()\n",
        "\n",
        "class ImageDataset(Dataset):\n",
        "    def __init__(self, image_dir, transform):\n",
        "        self.image_dir = image_dir\n",
        "        self.transform = transform\n",
        "        self.image_files = [f for f in os.listdir(image_dir) if f.endswith('.png') or f.endswith('.jpg')]\n",
        "        self.image_files.sort(key=lambda s: [int(c) if c.isdigit() else c.lower() for c in re.split(r'(\\d+)', s)])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = os.path.join(self.image_dir, self.image_files[idx])\n",
        "        image = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
        "        augmented = self.transform(image=image)\n",
        "        return augmented['image'], img_path\n",
        "\n",
        "\n",
        "def load_model(model_path):\n",
        "    model = UNET(in_channels=1, out_channels=1).to(DEVICE)\n",
        "    checkpoint = torch.load(model_path, map_location=DEVICE)\n",
        "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
        "    model.eval()\n",
        "    return model\n",
        "\n",
        "def get_transforms():\n",
        "    return A.Compose(\n",
        "        [\n",
        "            A.Resize(height=IMAGE_HEIGHT, width=IMAGE_WIDTH),\n",
        "            A.Normalize(mean=0.0, std=1.0, max_pixel_value=255.0),\n",
        "            ToTensorV2(),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "def generate_masks(model, dataloader):\n",
        "    all_masks = []\n",
        "    all_paths = []\n",
        "\n",
        "    for batch, paths in tqdm(dataloader, desc=\"Generating masks\"):\n",
        "        batch = batch.to(DEVICE)\n",
        "        with torch.no_grad():\n",
        "            predictions = torch.sigmoid(model(batch))\n",
        "            predictions = (predictions > 0.5).float()\n",
        "\n",
        "        masks = (predictions.cpu().numpy() * 255).astype(np.uint8)\n",
        "        all_masks.extend(masks)\n",
        "        all_paths.extend(paths)\n",
        "\n",
        "    return all_masks, all_paths\n",
        "\n",
        "def is_elliptical(mask, threshold=0.85):\n",
        "    contours, _ = cv2.findContours(mask.squeeze(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "    if not contours:\n",
        "        return False\n",
        "\n",
        "    largest_contour = max(contours, key=cv2.contourArea)\n",
        "\n",
        "    if len(largest_contour) < 5:\n",
        "        return False\n",
        "\n",
        "    ellipse = cv2.fitEllipse(largest_contour)\n",
        "    ellipse_mask = np.zeros_like(mask.squeeze())\n",
        "    cv2.ellipse(ellipse_mask, ellipse, 255, -1)\n",
        "\n",
        "    contour_area = cv2.contourArea(largest_contour)\n",
        "    ellipse_area = np.sum(ellipse_mask > 0)\n",
        "\n",
        "    if ellipse_area == 0:\n",
        "        return False\n",
        "\n",
        "    similarity = contour_area / ellipse_area\n",
        "    return similarity > threshold\n",
        "\n",
        "def process_images(image_paths, masks, centroids, eye_areas, model_start_time):\n",
        "    results = []\n",
        "    total_processed = 0\n",
        "    for idx, (image_path, mask, (cx, cy), eye_area) in enumerate(zip(image_paths, masks, centroids, eye_areas), 1):\n",
        "        image = cv2.imread(image_path)\n",
        "        if is_elliptical(mask):\n",
        "            red_overlay = np.zeros_like(image)\n",
        "            red_overlay[:,:,2] = 255\n",
        "            red_mask = cv2.bitwise_and(red_overlay, red_overlay, mask=mask.squeeze())\n",
        "            result = cv2.addWeighted(image, 1, red_mask, 0.5, 0)\n",
        "            eye_area = str(int(eye_area))\n",
        "            cx_display = f\"{cx:.2f}\"\n",
        "            cy_display = f\"{cy:.2f}\"\n",
        "        else:\n",
        "            result = image\n",
        "            eye_area = \"Undefined\"\n",
        "            cx_display = \"N/A\"\n",
        "            cy_display = \"N/A\"\n",
        "\n",
        "        total_processed += 1\n",
        "        current_time = time() - model_start_time\n",
        "        fps = total_processed / current_time if current_time > 0 else 0\n",
        "\n",
        "        cyan_color = (255, 255, 0)\n",
        "        font = cv2.FONT_HERSHEY_SIMPLEX\n",
        "        font_scale = 0.6\n",
        "        thickness = 2\n",
        "        report_text = f\"Area: {eye_area}, CX: {cx_display}, CY: {cy_display}, FPS: {fps:.2f}\"\n",
        "        cv2.putText(result, report_text, (20, 30), font, font_scale, cyan_color, thickness)\n",
        "\n",
        "        results.append(result)\n",
        "    return results\n",
        "\n",
        "def create_gif(image_list, output_path, duration=200):\n",
        "    images = [Image.fromarray(cv2.cvtColor(img, cv2.COLOR_BGR2RGB)) for img in image_list]\n",
        "    images[0].save(output_path, save_all=True, append_images=images[1:], duration=duration, loop=0)\n",
        "\n",
        "def calculate_centroids(masks):\n",
        "    centroids = []\n",
        "    for mask in masks:\n",
        "        M = cv2.moments(mask.squeeze())\n",
        "        if M[\"m00\"] != 0:\n",
        "            cx = M[\"m10\"] / M[\"m00\"]\n",
        "            cy = M[\"m01\"] / M[\"m00\"]\n",
        "        else:\n",
        "            cx, cy = 0, 0\n",
        "        centroids.append((cx, cy))\n",
        "    return centroids\n",
        "\n",
        "def calculate_eye_areas(masks):\n",
        "    return [np.sum(mask.squeeze() > 0) for mask in masks]\n",
        "\n",
        "def main():\n",
        "    model_path = \"best_model.pth.tar\"\n",
        "    image_dir = \"data/train/images\"\n",
        "    output_gif_path = \"output.gif\"\n",
        "\n",
        "    model = load_model(model_path)\n",
        "    transform = get_transforms()\n",
        "\n",
        "    dataset = ImageDataset(image_dir, transform)\n",
        "    dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS, pin_memory=True)\n",
        "\n",
        "    data_load_time = time()\n",
        "    masks, image_paths = generate_masks(model, dataloader)\n",
        "\n",
        "    centroids = calculate_centroids(masks)\n",
        "    eye_areas = calculate_eye_areas(masks)\n",
        "\n",
        "    model_start_time = time()\n",
        "    processed_images = process_images(image_paths, masks, centroids, eye_areas, model_start_time)\n",
        "\n",
        "    create_gif(processed_images, output_gif_path)\n",
        "\n",
        "    total_time = time() - model_start_time\n",
        "    total_images = len(masks)\n",
        "    average_fps = total_images / total_time\n",
        "    print(f\"GIF animation created at {output_gif_path}\")\n",
        "    print(f\"Total processing time: {total_time:.2f} seconds\")\n",
        "    print(f\"Total images processed: {total_images}\")\n",
        "    print(f\"Average FPS: {average_fps:.2f}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
